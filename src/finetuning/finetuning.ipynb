{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68aa2552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6d5f39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 08-23 16:32:31 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 08-23 16:32:31 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e8563d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Patching vLLM v1 graph capture\n",
      "Unsloth: Patching vLLM v0 graph capture\n",
      "==((====))==  Unsloth 2025.5.7: Fast Qwen3 patching. Transformers: 4.51.3. vLLM: 0.8.5.post1.\n",
      "   \\\\   /|    Quadro T1000 with Max-Q Design. Num GPUs = 1. Max memory: 4.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/Qwen3-0.6B with actual GPU utilization = 55.83%\n",
      "Unsloth: Your GPU has CUDA compute capability 7.5 with VRAM = 4.0 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 128.\n",
      "Unsloth: vLLM's KV Cache can use up to 1.25 GB. Also swap space = 0 GB.\n",
      "WARNING 08-23 16:32:27 [config.py:2972] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 08-23 16:32:35 [config.py:717] This model supports multiple tasks: {'classify', 'score', 'embed', 'generate', 'reward'}. Defaulting to 'generate'.\n",
      "WARNING 08-23 16:32:35 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
      "INFO 08-23 16:32:35 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='unsloth/Qwen3-0.6B', speculative_config=None, tokenizer='unsloth/Qwen3-0.6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/Qwen3-0.6B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"backend\":\"inductor\",\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":128}, use_cached_outputs=False, \n",
      "WARNING 08-23 16:32:36 [interface.py:314] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 08-23 16:32:36 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 08-23 16:32:36 [cuda.py:289] Using XFormers backend.\n",
      "INFO 08-23 16:32:36 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 08-23 16:32:36 [model_runner.py:1108] Starting to load model unsloth/Qwen3-0.6B...\n",
      "INFO 08-23 16:32:37 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "INFO 08-23 16:32:37 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4818eb873d324e588bd9bdb2458f3dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-23 16:32:38 [loader.py:458] Loading weights took 0.54 seconds\n",
      "INFO 08-23 16:32:38 [punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 08-23 16:32:38 [model_runner.py:1140] Model loading took 1.1649 GiB and 1.118414 seconds\n",
      "INFO 08-23 16:32:46 [worker.py:287] Memory profiling takes 8.28 seconds\n",
      "INFO 08-23 16:32:46 [worker.py:287] the current vLLM instance can use total_gpu_memory (4.00GiB) x gpu_memory_utilization (0.56) = 2.23GiB\n",
      "INFO 08-23 16:32:46 [worker.py:287] model weights take 1.16GiB; non_torch_memory takes 0.02GiB; PyTorch activation peak memory takes 0.70GiB; the rest of the memory reserved for KV Cache is 0.35GiB.\n",
      "INFO 08-23 16:32:47 [executor_base.py:112] # cuda blocks: 203, # CPU blocks: 0\n",
      "INFO 08-23 16:32:47 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 1.59x\n",
      "INFO 08-23 16:32:47 [vllm_utils.py:671] Unsloth: Running patched vLLM v0 `capture_model`.\n",
      "INFO 08-23 16:32:47 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb3ef6ccb5094ceaad978c5fe26bef2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Capturing CUDA graph shapes:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-23 16:32:59 [model_runner.py:1592] Graph capturing finished in 12 secs, took 0.24 GiB\n",
      "INFO 08-23 16:32:59 [vllm_utils.py:678] Unsloth: Patched vLLM v0 graph capture finished in 12 secs.\n",
      "INFO 08-23 16:32:59 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 21.54 seconds\n",
      "Unsloth: Just some info: will skip parsing ['post_feedforward_layernorm', 'pre_feedforward_layernorm']\n",
      "Unsloth: Just some info: will skip parsing ['post_feedforward_layernorm', 'pre_feedforward_layernorm']\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 2048  # Can increase for longer reasoning traces\n",
    "lora_rank = 32         # Larger rank = smarter, but slower\n",
    "\n",
    "# Load model + tokenizer with vLLM acceleration\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen3-0.6B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = False,       # False for LoRA 16bit\n",
    "    fast_inference = True,      # Enable vLLM fast inference\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.7, # Reduce if out of memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ffe24a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.globals import TRAINING_ROWS, TRAINING_COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "336f4e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "REASONING_START = \"<think>\"\n",
    "REASONING_END   = \"</think>\"\n",
    "SOLUTION_START  = \"<SOLUTION>\"\n",
    "SOLUTION_END    = \"</SOLUTION>\"\n",
    "\n",
    "SYSTEM_PROMPT = f\"\"\"You are a Minesweeper assistant.\n",
    "The game board is always {TRAINING_ROWS}x{TRAINING_COLS} in size.\n",
    "You will be given ONLY the current board state as input from the user.\n",
    "\n",
    "Your task: Suggest exactly ONE valid next move.\n",
    "\n",
    "Move format rules (must follow exactly one of these two):\n",
    "1. \"row col\"       â†’ to reveal a cell\n",
    "2. \"row col f\"     â†’ to flag a cell as a mine\n",
    "\n",
    "Board representation:\n",
    "- '*' means the tile has not been revealed yet.\n",
    "- Numbers 0â€“8 show how many mines are adjacent to that square.\n",
    "- The board will be displayed as a grid of symbols only.\n",
    "\n",
    "Here is an example board representation:\n",
    "\n",
    "* * * * * * * *\n",
    "* * * * * * * *\n",
    "* * * * * * * *\n",
    "* * * * * * * *\n",
    "* 2 1 1 1 * * *\n",
    "F 1 0 0 1 * * *\n",
    "1 1 0 0 1 3 * *\n",
    "0 0 0 0 0 2 * *\n",
    "\n",
    "Here, for example, the move \"6 6 f\" would flag the cell at row 6, column 6 as a mine.\n",
    "\n",
    "Constraints:\n",
    "- Row values are integers in [1, {TRAINING_ROWS}].\n",
    "- Column values are integers in [1, {TRAINING_COLS}].\n",
    "- Suggest only one valid move next with the format \"row col\" or \"row col f\".\n",
    "- Do NOT repeat or copy the board in your output.\n",
    "- Do NOT output anything except the next move.\n",
    "\n",
    "Here is the user board:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61c4303d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final move: <think>\n",
      "Okay, let's see. The user provided a Minesweeper board and I need to suggest exactly one valid next move. The board is 8x8, so I need to check each cell to see if there's a valid move.\n",
      "\n",
      "First, I'll look at the board. The user's input is a string of 8 rows, each with 8 elements. Let me parse it. The first row is \"0 0 1 * * * * *\". So row 0 (assuming rows start at 0) has cells 0, 0, 1, *, *, *, *, *.\n",
      "\n",
      "Now, the rules for moves: either \"row col\" (reveal) or \"row col f\" (flag). The task is to find exactly one valid move. Let's check each cell for possible moves.\n",
      "\n",
      "Looking at the first row, the first three cells are 0, 0, 1. The next cells are * (unrevealed). So, if I check the first row, the cells 0, 0, 1 are already revealed. The *s are unrevealed. So, maybe the next move is to reveal the *s. But which one?\n",
      "\n",
      "Wait, the user's input shows that the first row is \"0 0 1 * * * * *\". So, the first three cells are revealed. The rest are *s. So, the next move could be to reveal any of the *s. But which one? The user might have provided the board, and I need to pick one. But since the user hasn't specified which move to make, I need to choose one that's valid.\n",
      "\n",
      "Alternatively, maybe there's a cell that's a mine. Let me check. In the user's input, the fifth row is \"0 0 2 3 4 F 2 1\". The cell at row 4, column 5 (assuming columns start at 1) is F. So that's a flag. But maybe there's a mine in another cell.\n",
      "\n",
      "Looking at the board, perhaps the cell at row 4, column 5 is a flag. But if that's a flag, then revealing it would be a valid move. But the user might have intended to flag it. However, the move format allows either. So maybe the correct move is to flag that cell. But I need to check if that's a valid move.\n",
      "\n",
      "Alternatively, maybe there's a cell that's a mine. Let me check. The user's input shows that in row 4, column 5 is F. So that's a flag. But if that's a mine, then the move would be to flag it. But since the user's input shows that, perhaps that's the correct move. However, the user might have intended to flag it. But I need to make sure.\n",
      "\n",
      "Alternatively, maybe there's a cell that's a mine. Let me check. For example, in row 4, column 5 is F. So that's a flag. But if that's a mine, then the move would be to flag it. But since the user's input shows that, perhaps that's the correct move. However, the user might have intended to flag it. But since the move format allows either, I can choose either.\n",
      "\n",
      "But the user's instruction says to suggest exactly one valid move. So perhaps the correct move is to flag the cell at row 4, column 5. But I need to confirm.\n",
      "\n",
      "Alternatively, maybe there's another cell. Let me check. For example, in row 4, column 5 is F. So that's a flag. If that's a mine, then the move would be to flag it. But since the user's input shows that, perhaps that's the correct move. However, the user might have intended to flag it. But since the move format allows either, I can choose that.\n",
      "\n",
      "Alternatively, maybe there's a cell that's a mine. Let me check again. The user's input shows that in row 4, column 5 is F. So that's a flag. So the move would be to flag that cell. Therefore, the next move is \"4 5 f\".\n",
      "\n",
      "But I need to make sure that this is a valid move. Since the cell is flagged, it's a mine, so revealing it would be a valid move. So the answer would be \"4 5 f\".\n",
      "\n",
      "Alternatively, maybe there's another cell. Let me check another cell. For example, in row 4, column 5 is F. So that's a flag. So the move is to flag it. Therefore, the correct answer is \"4 5 f\".\n",
      "</think>\n",
      "\n",
      "4 5 f\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": \"\"\"\n",
    "0 0 1 * * * * *\n",
    "0 0 1 * * * * *\n",
    "0 0 2 * * * * *\n",
    "0 0 1 F 2 * * *\n",
    "0 0 2 3 4 F 2 1\n",
    "0 0 1 F F 2 1 0\n",
    "0 0 1 2 2 1 0 0\n",
    "0 0 0 0 0 0 0 0\n",
    "\"\"\"}\n",
    "]\n",
    "\n",
    "# Apply chat template, enabling thinking mode\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True   # key part for <think> sections\n",
    ")\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate\n",
    "generated_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=1024,\n",
    ")\n",
    "\n",
    "# Extract only the new tokens\n",
    "output_ids = generated_ids[0][len(inputs.input_ids[0]):].tolist()\n",
    "decoded_output = tokenizer.decode(output_ids, skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "\n",
    "# Extract solution block\n",
    "import re\n",
    "match = re.search(r\"<SOLUTION>(.*?)</SOLUTION>\", decoded_output, re.DOTALL)\n",
    "if match:\n",
    "    move = match.group(1).strip()\n",
    "else:\n",
    "    move = decoded_output  # fallback if tags missing\n",
    "\n",
    "print(\"final move:\", move)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MinesweeperGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
