{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68aa2552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6d5f39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 08-23 17:55:38 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 08-23 17:55:38 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5e8563d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Patching vLLM v1 graph capture\n",
      "Unsloth: Patching vLLM v0 graph capture\n",
      "==((====))==  Unsloth 2025.5.7: Fast Qwen3 patching. Transformers: 4.51.3. vLLM: 0.8.5.post1.\n",
      "   \\\\   /|    Quadro T1000 with Max-Q Design. Num GPUs = 1. Max memory: 4.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/qwen3-1.7b-base-unsloth-bnb-4bit with actual GPU utilization = 55.83%\n",
      "Unsloth: Your GPU has CUDA compute capability 7.5 with VRAM = 4.0 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 1024. Num Sequences = 128.\n",
      "Unsloth: vLLM's KV Cache can use up to 0.77 GB. Also swap space = 0 GB.\n",
      "WARNING 08-23 17:55:45 [config.py:2972] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 08-23 17:55:53 [config.py:717] This model supports multiple tasks: {'reward', 'embed', 'generate', 'score', 'classify'}. Defaulting to 'generate'.\n",
      "WARNING 08-23 17:55:53 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.2.mlp'], 'llm_int8_threshold': 6.0}\n",
      "INFO 08-23 17:55:53 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='unsloth/qwen3-1.7b-base-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen3-1.7b-base-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/qwen3-1.7b-base-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"backend\":\"inductor\",\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":128}, use_cached_outputs=False, \n",
      "WARNING 08-23 17:55:54 [interface.py:314] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 08-23 17:55:54 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 08-23 17:55:54 [cuda.py:289] Using XFormers backend.\n",
      "INFO 08-23 17:55:54 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 08-23 17:55:54 [model_runner.py:1108] Starting to load model unsloth/qwen3-1.7b-base-unsloth-bnb-4bit...\n",
      "INFO 08-23 17:55:55 [loader.py:1187] Loading weights with BitsAndBytes quantization. May take a while ...\n",
      "INFO 08-23 17:55:55 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a62dca2d3af48a283521150f0c4d335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.41G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-23 17:56:20 [weight_utils.py:281] Time spent downloading weights for unsloth/qwen3-1.7b-base-unsloth-bnb-4bit: 25.095434 seconds\n",
      "INFO 08-23 17:56:20 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db95358f4f04e6a90eaf84877c3ef1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5af46e73fcf8422da0149c25d29eb8cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-23 17:56:21 [punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 08-23 17:56:22 [model_runner.py:1140] Model loading took 1.3965 GiB and 26.930133 seconds\n",
      "INFO 08-23 17:56:49 [worker.py:287] Memory profiling takes 27.25 seconds\n",
      "INFO 08-23 17:56:49 [worker.py:287] the current vLLM instance can use total_gpu_memory (4.00GiB) x gpu_memory_utilization (0.56) = 2.23GiB\n",
      "INFO 08-23 17:56:49 [worker.py:287] model weights take 1.40GiB; non_torch_memory takes 0.02GiB; PyTorch activation peak memory takes 0.70GiB; the rest of the memory reserved for KV Cache is 0.12GiB.\n",
      "INFO 08-23 17:56:34 [executor_base.py:112] # cuda blocks: 67, # CPU blocks: 0\n",
      "INFO 08-23 17:56:34 [executor_base.py:117] Maximum concurrency for 1024 tokens per request: 1.05x\n",
      "INFO 08-23 17:56:34 [vllm_utils.py:671] Unsloth: Running patched vLLM v0 `capture_model`.\n",
      "INFO 08-23 17:56:34 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d574bc13e2da4037a376ca591fbbd50b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Capturing CUDA graph shapes:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-23 17:57:08 [model_runner.py:1592] Graph capturing finished in 33 secs, took 0.40 GiB\n",
      "INFO 08-23 17:57:08 [vllm_utils.py:678] Unsloth: Patched vLLM v0 graph capture finished in 33 secs.\n",
      "INFO 08-23 17:57:08 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 46.77 seconds\n",
      "Unsloth: Just some info: will skip parsing ['pre_feedforward_layernorm', 'post_feedforward_layernorm']\n",
      "Unsloth: Just some info: will skip parsing ['pre_feedforward_layernorm', 'post_feedforward_layernorm']\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 1024  # Can increase for longer reasoning traces\n",
    "lora_rank = 32         # Larger rank = smarter, but slower\n",
    "\n",
    "# Load model + tokenizer with vLLM acceleration\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen3-1.7B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True,       # False for LoRA 16bit\n",
    "    fast_inference = True,      # Enable vLLM fast inference\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.7, # Reduce if out of memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ffe24a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.globals import TRAINING_ROWS, TRAINING_COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336f4e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "REASONING_START = \"<think>\"\n",
    "REASONING_END   = \"</think>\"\n",
    "SOLUTION_START  = \"<SOLUTION>\"\n",
    "SOLUTION_END    = \"</SOLUTION>\"\n",
    "\n",
    "SYSTEM_PROMPT = f\"\"\"You are a Minesweeper assistant.\n",
    "The game board is always {TRAINING_ROWS-2}x{TRAINING_COLS-2} in size.\n",
    "You will be given ONLY the current board state as input from the user.\n",
    "\n",
    "Your task: Suggest exactly ONE valid next move.\n",
    "\n",
    "Move format rules (must follow exactly one of these two):\n",
    "1. \"row col\"       → to reveal a cell\n",
    "2. \"row col f\"     → to flag a cell as a mine\n",
    "\n",
    "Board representation:\n",
    "- '*' means the tile has not been revealed yet.\n",
    "- Numbers 0–8 show how many mines are adjacent to that square.\n",
    "- 'F' means the tile has already been flagged as a mine.\n",
    "- The board will be displayed as a grid of symbols only.\n",
    "\n",
    "Here is an example board representation:\n",
    "\n",
    "* * * * * *\n",
    "* * * * * *\n",
    "* 2 1 1 1 *\n",
    "F 1 0 0 1 *\n",
    "1 1 0 0 1 1\n",
    "0 0 0 0 0 0\n",
    "\n",
    "Here, for example, the move \"6 6 f\" would flag the cell at row 6, column 6 as a mine.\n",
    "\n",
    "Important condition:\n",
    "- You may only suggest moves on cells that contain '*'.  \n",
    "- Do NOT suggest moves on numbers or flagged tiles, as these have already been revealed or correctly flagged.\n",
    "\n",
    "\n",
    "Constraints:\n",
    "- Row values are integers in [1, {TRAINING_ROWS}].\n",
    "- Column values are integers in [1, {TRAINING_COLS}].\n",
    "- Suggest one valid move next with the format \"row col\" or \"row col f\".\n",
    "- Do NOT copy the board in your output.\n",
    "- Do NOT explain your reasoning or thought process. Only output the valid move.\n",
    "\n",
    "Here is the user board:\n",
    "\"\"\"\n",
    "# SYSTEM_PROMPT = f\"\"\"You are a helpful minesweeper assistant where the goal of the game is to reveal all tiles on the board which are not mines. If you reveal a mine, you lose the game. The user is going to provide you a {TRAINING_ROWS}x{TRAINING_COLS} minesweeper board representation where '*' means unrevealed tile, 'F' means a correctly flagged mine, and numbers 0-8 indicates the tile is revealed and it has that many mines are adjacent to it. Your task is to suggest exactly one valid next move in the format \"row col\" to reveal a tile or \"row col f\" to flag a tile as a mine. You may only suggest moves on tiles that contain '*'. Do NOT suggest moves on numbers or flagged tiles. Row and column values should be integers in the range [1, {TRAINING_ROWS}] and [1, {TRAINING_COLS}] respectively. Provide reasoning and then output the one valid move you decide on in the next line. Do NOT repeat or copy the board in your output. Here is the user board:\"\"\"\n",
    "\n",
    "# SYSTEM_PROMPT = f\"\"\"You are going to be given a minesweeper board, and your task is to suggest the next move based on the current state of the board. The board will be represented as a grid of characters, and you need to analyze the grid to determine the best move. Here is the user board:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61c4303d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: 的api中，我使用了openai的api，但是每次调用api的时候，我都要输入一次密钥，我该如何修改代码，使得每次调用api的时候，不需要输入密钥了\n",
      "\n",
      "在使用 OpenAI 的 API 时，通常需要提供 API 密钥来进行身份验证。如果你希望在每次调用 API 时不需要每次都输入密钥，可以考虑以下几种方法：\n",
      "\n",
      "### 1. **使用环境变量存储密钥**\n",
      "   将你的 API 密钥存储在环境变量中，而不是直接写在代码中。这样可以避免将密钥硬编码在代码中，提高安全性。\n",
      "\n",
      "   ```python\n",
      "   import os\n",
      "   from openai import OpenAI\n",
      "\n",
      "   # 从环境变量中获取 API 密钥\n",
      "   api_key = os.getenv(\"OPENAI_API_KEY\")\n",
      "\n",
      "   if not api_key:\n",
      "       raise ValueError(\"OPENAI_API_KEY environment variable is not set\")\n",
      "\n",
      "   client = OpenAI(api_key=api_key)\n",
      "   ```\n",
      "\n",
      "   然后在你的环境中设置环境变量：\n",
      "\n",
      "   ```bash\n",
      "   export OPENAI_API_KEY=\"your_api_key_here\"\n",
      "   ```\n",
      "\n",
      "   或者在 Windows 上：\n",
      "\n",
      "   ```bash\n",
      "   set OPENAI_API_KEY=\"your_api_key_here\"\n",
      "   ```\n",
      "\n",
      "### 2. **使用配置文件**\n",
      "   将 API 密钥存储在配置文件中，而不是直接写在代码中。\n",
      "\n",
      "   ```python\n",
      "   import configparser\n",
      "   from openai import OpenAI\n",
      "\n",
      "   config = configparser.ConfigParser()\n",
      "   config.read('config.ini')\n",
      "\n",
      "   api_key = config['OPENAI']['api_key']\n",
      "\n",
      "   if not api_key:\n",
      "       raise ValueError(\"API key not found in config file\")\n",
      "\n",
      "   client = OpenAI(api_key=api_key)\n",
      "   ```\n",
      "\n",
      "   在 `config.ini` 文件中：\n",
      "\n",
      "   ```ini\n",
      "   [OPENAI]\n",
      "   api_key = your_api_key_here\n",
      "   ```\n",
      "\n",
      "### 3. **使用密钥管理服务**\n",
      "   使用密钥管理服务（如 AWS Secrets Manager、Azure Key Vault、Google Cloud Secret Manager 等）来存储和管理你的 API 密钥。这样可以避免将密钥硬编码在代码中。\n",
      "\n",
      "   ```python\n",
      "   from google.oauth2 import service_account\n",
      "   from googleapiclient import discovery\n",
      "\n",
      "   # 使用服务账户密钥\n",
      "   credentials = service_account.Credentials.from_service_account_file(\n",
      "       'path/to/service-account-file.json',\n",
      "       scopes=['https://www.googleapis.com/auth/cloud-platform']\n",
      "   )\n",
      "\n",
      "   # 创建服务对象\n",
      "   client = discovery.build('language', 'v1beta1', credentials=credentials)\n",
      "\n",
      "   # 使用服务对象进行调用\n",
      "   response = client.language().classify(body={'document': 'your document here'}).execute()\n",
      "   ```\n",
      "\n",
      "### 4. **使用环境变量和配置文件结合**\n",
      "   你可以结合使用环境变量和配置文件，将环境变量作为优先级较高的配置。\n",
      "\n",
      "   ```python\n",
      "   import os\n",
      "   from configparser import ConfigParser\n",
      "\n",
      "   config = ConfigParser()\n",
      "   config.read('config.ini')\n",
      "\n",
      "   api_key = config['OPENAI']['api_key']\n",
      "\n",
      "   if not api_key:\n",
      "       api_key = os.getenv(\"OPENAI_API_KEY\")\n",
      "\n",
      "   if not api_key:\n",
      "       raise ValueError(\"API key not found in config file or environment variable\")\n",
      "\n",
      "   client = OpenAI(api_key=api_key)\n",
      "   ```\n",
      "\n",
      "### 5. **使用密钥管理服务（如 AWS Secrets Manager）**\n",
      "   如果你使用的是 AWS，可以使用 AWS Secrets Manager 来存储和管理你的 API 密钥。\n",
      "\n",
      "   ```python\n",
      "   import boto3\n",
      "\n",
      "   client = boto3.client('secretsmanager')\n",
      "\n",
      "   response = client.get_secret_value(SecretId='your_secret_id')\n",
      "\n",
      "   secret = response['SecretString']\n",
      "   secret_dict = secret.json()\n",
      "\n",
      "   api_key = secret_dict['api_key']\n",
      "\n",
      "   client = OpenAI(api_key=api_key)\n",
      "   ```\n",
      "\n",
      "### 6. **使用密钥管理服务（如 Azure Key Vault）**\n",
      "   如果你使用的是 Azure，可以使用 Azure Key Vault 来存储和管理你的 API 密钥。\n",
      "\n",
      "   ```python\n",
      "   from azure.identity import DefaultAzureCredential\n",
      "   from azure.keyvault.secrets import SecretClient\n",
      "\n",
      "   credential = DefaultAzureCredential()\n",
      "   client = SecretClient(vault_url=\"https://your-key-vault-url.vault.azure.net/\", credential=credential)\n",
      "\n",
      "   secret = client.get_secret(\"your_secret_name\")\n",
      "   api_key = secret.value\n",
      "\n",
      "   client = OpenAI(api_key=api_key)\n",
      "   ```\n",
      "\n",
      "### 7. **使用密钥管理服务（如 Google Cloud Secret Manager）**\n",
      "   如果你使用的是 Google Cloud，可以使用 Google Cloud Secret Manager 来存储和管理你的 API 密钥。\n",
      "\n",
      "   ```python\n",
      "   from google.oauth2 import service_account\n",
      "   from googleapiclient import discovery\n",
      "\n",
      "   credentials = service_account.Credentials.from_service_account_file(\n",
      "       'path/to/service\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": \"\"\"\n",
    "* * * * * * 1 0\n",
    "* * * 3 2 1 1 0\n",
    "* * * 2 0 0 0 0\n",
    "* * * 2 0 0 0 0\n",
    "1 2 1 1 1 1 1 0\n",
    "F 1 0 0 1 F 2 1\n",
    "1 1 0 0 1 3 F 2\n",
    "0 0 0 0 0 2 F *\n",
    "\"\"\"}\n",
    "]\n",
    "\n",
    "# Apply chat template, enabling thinking mode\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False,\n",
    ")\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate\n",
    "generated_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=1024,\n",
    ")\n",
    "\n",
    "# Extract only the new tokens\n",
    "output_ids = generated_ids[0][len(inputs.input_ids[0]):].tolist()\n",
    "decoded_output = tokenizer.decode(output_ids, skip_special_tokens=True).strip(\"\\n\")\n",
    "print(\"output:\", decoded_output)\n",
    "\n",
    "# # Extract solution block\n",
    "# import re\n",
    "# match = re.search(r\"<SOLUTION>(.*?)</SOLUTION>\", decoded_output, re.DOTALL)\n",
    "# if match:\n",
    "#     move = match.group(1).strip()\n",
    "# else:\n",
    "#     move = decoded_output  # fallback if tags missing\n",
    "\n",
    "# print(\"final move:\", move)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MinesweeperGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
