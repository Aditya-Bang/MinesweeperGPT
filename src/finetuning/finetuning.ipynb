{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68aa2552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6d5f39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 08-23 19:13:17 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 08-23 19:13:17 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5e8563d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Patching vLLM v1 graph capture\n",
      "Unsloth: Patching vLLM v0 graph capture\n",
      "==((====))==  Unsloth 2025.5.7: Fast Qwen3 patching. Transformers: 4.51.3. vLLM: 0.8.5.post1.\n",
      "   \\\\   /|    Quadro T1000 with Max-Q Design. Num GPUs = 1. Max memory: 4.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/qwen3-1.7b-unsloth-bnb-4bit with actual GPU utilization = 55.83%\n",
      "Unsloth: Your GPU has CUDA compute capability 7.5 with VRAM = 4.0 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 1024. Num Sequences = 128.\n",
      "Unsloth: vLLM's KV Cache can use up to 0.77 GB. Also swap space = 0 GB.\n",
      "WARNING 08-23 19:13:25 [config.py:2972] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 08-23 19:13:35 [config.py:717] This model supports multiple tasks: {'classify', 'reward', 'score', 'generate', 'embed'}. Defaulting to 'generate'.\n",
      "WARNING 08-23 19:13:36 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.2.mlp'], 'llm_int8_threshold': 6.0}\n",
      "INFO 08-23 19:13:36 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='unsloth/qwen3-1.7b-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen3-1.7b-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/qwen3-1.7b-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"backend\":\"inductor\",\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":128}, use_cached_outputs=False, \n",
      "WARNING 08-23 19:13:36 [interface.py:314] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 08-23 19:13:36 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 08-23 19:13:36 [cuda.py:289] Using XFormers backend.\n",
      "INFO 08-23 19:13:37 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 08-23 19:13:37 [model_runner.py:1108] Starting to load model unsloth/qwen3-1.7b-unsloth-bnb-4bit...\n",
      "INFO 08-23 19:13:37 [loader.py:1187] Loading weights with BitsAndBytes quantization. May take a while ...\n",
      "INFO 08-23 19:13:38 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "INFO 08-23 19:13:38 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c3fe4a623df4a6a9ccdcea5d4276c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d09d097caae44fdb50f8cf53942876e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-23 19:13:40 [punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 08-23 19:13:40 [model_runner.py:1140] Model loading took 1.3985 GiB and 3.064188 seconds\n",
      "INFO 08-23 19:13:54 [worker.py:287] Memory profiling takes 13.24 seconds\n",
      "INFO 08-23 19:13:54 [worker.py:287] the current vLLM instance can use total_gpu_memory (4.00GiB) x gpu_memory_utilization (0.56) = 2.23GiB\n",
      "INFO 08-23 19:13:54 [worker.py:287] model weights take 1.40GiB; non_torch_memory takes 0.02GiB; PyTorch activation peak memory takes 0.70GiB; the rest of the memory reserved for KV Cache is 0.11GiB.\n",
      "INFO 08-23 19:13:54 [executor_base.py:112] # cuda blocks: 66, # CPU blocks: 0\n",
      "INFO 08-23 19:13:54 [executor_base.py:117] Maximum concurrency for 1024 tokens per request: 1.03x\n",
      "INFO 08-23 19:13:54 [vllm_utils.py:671] Unsloth: Running patched vLLM v0 `capture_model`.\n",
      "INFO 08-23 19:13:54 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c7c03b879b44c48b438dd49f8f0ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Capturing CUDA graph shapes:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-23 19:14:28 [model_runner.py:1592] Graph capturing finished in 34 secs, took 0.40 GiB\n",
      "INFO 08-23 19:14:28 [vllm_utils.py:678] Unsloth: Patched vLLM v0 graph capture finished in 34 secs.\n",
      "INFO 08-23 19:14:28 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 47.91 seconds\n",
      "Unsloth: Just some info: will skip parsing ['pre_feedforward_layernorm', 'post_feedforward_layernorm']\n",
      "Unsloth: Just some info: will skip parsing ['pre_feedforward_layernorm', 'post_feedforward_layernorm']\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 1024  # Can increase for longer reasoning traces\n",
    "lora_rank = 32         # Larger rank = smarter, but slower\n",
    "\n",
    "# Load model + tokenizer with vLLM acceleration\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen3-1.7B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True,       # False for LoRA 16bit\n",
    "    fast_inference = True,      # Enable vLLM fast inference\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.7, # Reduce if out of memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ffe24a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.globals import TRAINING_ROWS, TRAINING_COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "336f4e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "REASONING_START = \"<think>\"\n",
    "REASONING_END   = \"</think>\"\n",
    "SOLUTION_START  = \"<SOLUTION>\"\n",
    "SOLUTION_END    = \"</SOLUTION>\"\n",
    "\n",
    "SYSTEM_PROMPT = f\"\"\"You are a Minesweeper assistant.\n",
    "The game board is always {TRAINING_ROWS}x{TRAINING_COLS} in size.\n",
    "You will be given ONLY the current board state as input from the user.\n",
    "\n",
    "Your task: Suggest exactly ONE valid next move for the minesweeper board given by the user.\n",
    "\n",
    "Move format rules (must follow exactly one of these two):\n",
    "1. \"row: NUM, col: NUM, action: reveal\"       â†’ to reveal a cell\n",
    "2. \"row: NUM, col: NUM, action: flag\"         â†’ to flag a cell as a mine\n",
    "\n",
    "Board representation:\n",
    "- '*' means the tile has not been revealed yet.\n",
    "- Numbers 0â€“8 show how many mines are adjacent to that square.\n",
    "- 'F' means the tile has already been flagged as a mine.\n",
    "- The board will be displayed as a grid of symbols only.\n",
    "\n",
    "Important condition:\n",
    "- You may only suggest moves on cells that contain '*'.  \n",
    "- Do NOT suggest moves on numbers or flagged tiles, as these have already been revealed or correctly flagged.\n",
    "\n",
    "Summary:\n",
    "- Suggest one valid move next with the format \"row: NUM, col: NUM, action: reveal\" or \"row: NUM, col: NUM, action: flag\", where NUM is an integer in the range [1, {TRAINING_ROWS}] for rows and [1, {TRAINING_COLS}] for columns.\n",
    "- Do NOT explain your reasoning or thought process. Only output the valid move.\n",
    "\"\"\"\n",
    "# SYSTEM_PROMPT = f\"\"\"You are a helpful minesweeper assistant where the goal of the game is to reveal all tiles on the board which are not mines. If you reveal a mine, you lose the game. The user is going to provide you a {TRAINING_ROWS}x{TRAINING_COLS} minesweeper board representation where '*' means unrevealed tile, 'F' means a correctly flagged mine, and numbers 0-8 indicates the tile is revealed and it has that many mines are adjacent to it. Your task is to suggest exactly one valid next move in the format \"row col\" to reveal a tile or \"row col f\" to flag a tile as a mine. You may only suggest moves on tiles that contain '*'. Do NOT suggest moves on numbers or flagged tiles. Row and column values should be integers in the range [1, {TRAINING_ROWS}] and [1, {TRAINING_COLS}] respectively. Provide reasoning and then output the one valid move you decide on in the next line. Do NOT repeat or copy the board in your output. Here is the user board:\"\"\"\n",
    "\n",
    "# SYSTEM_PROMPT = f\"\"\"You are going to be given a minesweeper board, and your task is to suggest the next move based on the current state of the board. The board will be represented as a grid of characters, and you need to analyze the grid to determine the best move. Here is the user board:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61c4303d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: row: 1, col: 5, action: reveal\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "#     {\"role\": \"user\", \"content\": \"\"\"\n",
    "# * * * * * *\n",
    "# * * F * * *\n",
    "# * * 3 3 4 *\n",
    "# 1 1 1 0 3 *\n",
    "# 0 0 0 0 3 *\n",
    "# 0 0 0 0 2 *\n",
    "# \"\"\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"row: 2, col: 4, action: flag\"},\n",
    "#     {\"role\": \"user\", \"content\": \"\"\"\n",
    "# 0 1 F 1 2 F\n",
    "# 0 1 1 1 2 F\n",
    "# 0 0 0 0 2 2\n",
    "# 1 2 1 1 1 F\n",
    "# * * F 2 3 3\n",
    "# * * * * F F\n",
    "# \"\"\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"row: 6, col: 3, action: reveal\"},\n",
    "    {\"role\": \"user\", \"content\": \"\"\"\n",
    "0 0 0 0 0 0\n",
    "2 2 1 0 1 1\n",
    "F F 1 0 2 F\n",
    "3 3 2 0 3 F\n",
    "1 F 2 1 4 F\n",
    "* * * * * *\n",
    "\"\"\"}\n",
    "]\n",
    "\n",
    "# Apply chat template, enabling thinking mode\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False,\n",
    ")\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate\n",
    "generated_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "\n",
    "# Extract only the new tokens\n",
    "output_ids = generated_ids[0][len(inputs.input_ids[0]):].tolist()\n",
    "decoded_output = tokenizer.decode(output_ids, skip_special_tokens=True).strip(\"\\n\")\n",
    "print(\"output:\", decoded_output)\n",
    "\n",
    "# # Extract solution block\n",
    "# import re\n",
    "# match = re.search(r\"<SOLUTION>(.*?)</SOLUTION>\", decoded_output, re.DOTALL)\n",
    "# if match:\n",
    "#     move = match.group(1).strip()\n",
    "# else:\n",
    "#     move = decoded_output  # fallback if tags missing\n",
    "\n",
    "# print(\"final move:\", move)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MinesweeperGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
