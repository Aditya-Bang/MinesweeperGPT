{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68aa2552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6d5f39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 08-23 17:55:38 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 08-23 17:55:38 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5e8563d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Patching vLLM v1 graph capture\n",
      "Unsloth: Patching vLLM v0 graph capture\n",
      "==((====))==  Unsloth 2025.5.7: Fast Qwen3 patching. Transformers: 4.51.3. vLLM: 0.8.5.post1.\n",
      "   \\\\   /|    Quadro T1000 with Max-Q Design. Num GPUs = 1. Max memory: 4.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/qwen3-1.7b-base-unsloth-bnb-4bit with actual GPU utilization = 55.83%\n",
      "Unsloth: Your GPU has CUDA compute capability 7.5 with VRAM = 4.0 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 1024. Num Sequences = 128.\n",
      "Unsloth: vLLM's KV Cache can use up to 0.77 GB. Also swap space = 0 GB.\n",
      "WARNING 08-23 17:55:45 [config.py:2972] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 08-23 17:55:53 [config.py:717] This model supports multiple tasks: {'reward', 'embed', 'generate', 'score', 'classify'}. Defaulting to 'generate'.\n",
      "WARNING 08-23 17:55:53 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.2.mlp'], 'llm_int8_threshold': 6.0}\n",
      "INFO 08-23 17:55:53 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='unsloth/qwen3-1.7b-base-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen3-1.7b-base-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/qwen3-1.7b-base-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"backend\":\"inductor\",\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":128}, use_cached_outputs=False, \n",
      "WARNING 08-23 17:55:54 [interface.py:314] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 08-23 17:55:54 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 08-23 17:55:54 [cuda.py:289] Using XFormers backend.\n",
      "INFO 08-23 17:55:54 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 08-23 17:55:54 [model_runner.py:1108] Starting to load model unsloth/qwen3-1.7b-base-unsloth-bnb-4bit...\n",
      "INFO 08-23 17:55:55 [loader.py:1187] Loading weights with BitsAndBytes quantization. May take a while ...\n",
      "INFO 08-23 17:55:55 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a62dca2d3af48a283521150f0c4d335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.41G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-23 17:56:20 [weight_utils.py:281] Time spent downloading weights for unsloth/qwen3-1.7b-base-unsloth-bnb-4bit: 25.095434 seconds\n",
      "INFO 08-23 17:56:20 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db95358f4f04e6a90eaf84877c3ef1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5af46e73fcf8422da0149c25d29eb8cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-23 17:56:21 [punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 08-23 17:56:22 [model_runner.py:1140] Model loading took 1.3965 GiB and 26.930133 seconds\n",
      "INFO 08-23 17:56:49 [worker.py:287] Memory profiling takes 27.25 seconds\n",
      "INFO 08-23 17:56:49 [worker.py:287] the current vLLM instance can use total_gpu_memory (4.00GiB) x gpu_memory_utilization (0.56) = 2.23GiB\n",
      "INFO 08-23 17:56:49 [worker.py:287] model weights take 1.40GiB; non_torch_memory takes 0.02GiB; PyTorch activation peak memory takes 0.70GiB; the rest of the memory reserved for KV Cache is 0.12GiB.\n",
      "INFO 08-23 17:56:34 [executor_base.py:112] # cuda blocks: 67, # CPU blocks: 0\n",
      "INFO 08-23 17:56:34 [executor_base.py:117] Maximum concurrency for 1024 tokens per request: 1.05x\n",
      "INFO 08-23 17:56:34 [vllm_utils.py:671] Unsloth: Running patched vLLM v0 `capture_model`.\n",
      "INFO 08-23 17:56:34 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d574bc13e2da4037a376ca591fbbd50b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Capturing CUDA graph shapes:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-23 17:57:08 [model_runner.py:1592] Graph capturing finished in 33 secs, took 0.40 GiB\n",
      "INFO 08-23 17:57:08 [vllm_utils.py:678] Unsloth: Patched vLLM v0 graph capture finished in 33 secs.\n",
      "INFO 08-23 17:57:08 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 46.77 seconds\n",
      "Unsloth: Just some info: will skip parsing ['pre_feedforward_layernorm', 'post_feedforward_layernorm']\n",
      "Unsloth: Just some info: will skip parsing ['pre_feedforward_layernorm', 'post_feedforward_layernorm']\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 1024  # Can increase for longer reasoning traces\n",
    "lora_rank = 32         # Larger rank = smarter, but slower\n",
    "\n",
    "# Load model + tokenizer with vLLM acceleration\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen3-1.7B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True,       # False for LoRA 16bit\n",
    "    fast_inference = True,      # Enable vLLM fast inference\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.7, # Reduce if out of memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ffe24a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.globals import TRAINING_ROWS, TRAINING_COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336f4e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "REASONING_START = \"<think>\"\n",
    "REASONING_END   = \"</think>\"\n",
    "SOLUTION_START  = \"<SOLUTION>\"\n",
    "SOLUTION_END    = \"</SOLUTION>\"\n",
    "\n",
    "SYSTEM_PROMPT = f\"\"\"You are a Minesweeper assistant.\n",
    "The game board is always {TRAINING_ROWS-2}x{TRAINING_COLS-2} in size.\n",
    "You will be given ONLY the current board state as input from the user.\n",
    "\n",
    "Your task: Suggest exactly ONE valid next move.\n",
    "\n",
    "Move format rules (must follow exactly one of these two):\n",
    "1. \"row col\"       â†’ to reveal a cell\n",
    "2. \"row col f\"     â†’ to flag a cell as a mine\n",
    "\n",
    "Board representation:\n",
    "- '*' means the tile has not been revealed yet.\n",
    "- Numbers 0â€“8 show how many mines are adjacent to that square.\n",
    "- 'F' means the tile has already been flagged as a mine.\n",
    "- The board will be displayed as a grid of symbols only.\n",
    "\n",
    "Here is an example board representation:\n",
    "\n",
    "* * * * * *\n",
    "* * * * * *\n",
    "* 2 1 1 1 *\n",
    "F 1 0 0 1 *\n",
    "1 1 0 0 1 1\n",
    "0 0 0 0 0 0\n",
    "\n",
    "Here, for example, the move \"6 6 f\" would flag the cell at row 6, column 6 as a mine.\n",
    "\n",
    "Important condition:\n",
    "- You may only suggest moves on cells that contain '*'.  \n",
    "- Do NOT suggest moves on numbers or flagged tiles, as these have already been revealed or correctly flagged.\n",
    "\n",
    "\n",
    "Constraints:\n",
    "- Row values are integers in [1, {TRAINING_ROWS}].\n",
    "- Column values are integers in [1, {TRAINING_COLS}].\n",
    "- Suggest one valid move next with the format \"row col\" or \"row col f\".\n",
    "- Do NOT copy the board in your output.\n",
    "- Do NOT explain your reasoning or thought process. Only output the valid move.\n",
    "\n",
    "Here is the user board:\n",
    "\"\"\"\n",
    "# SYSTEM_PROMPT = f\"\"\"You are a helpful minesweeper assistant where the goal of the game is to reveal all tiles on the board which are not mines. If you reveal a mine, you lose the game. The user is going to provide you a {TRAINING_ROWS}x{TRAINING_COLS} minesweeper board representation where '*' means unrevealed tile, 'F' means a correctly flagged mine, and numbers 0-8 indicates the tile is revealed and it has that many mines are adjacent to it. Your task is to suggest exactly one valid next move in the format \"row col\" to reveal a tile or \"row col f\" to flag a tile as a mine. You may only suggest moves on tiles that contain '*'. Do NOT suggest moves on numbers or flagged tiles. Row and column values should be integers in the range [1, {TRAINING_ROWS}] and [1, {TRAINING_COLS}] respectively. Provide reasoning and then output the one valid move you decide on in the next line. Do NOT repeat or copy the board in your output. Here is the user board:\"\"\"\n",
    "\n",
    "# SYSTEM_PROMPT = f\"\"\"You are going to be given a minesweeper board, and your task is to suggest the next move based on the current state of the board. The board will be represented as a grid of characters, and you need to analyze the grid to determine the best move. Here is the user board:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61c4303d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: çš„apiä¸­ï¼Œæˆ‘ä½¿ç”¨äº†openaiçš„apiï¼Œä½†æ˜¯æ¯æ¬¡è°ƒç”¨apiçš„æ—¶å€™ï¼Œæˆ‘éƒ½è¦è¾“å…¥ä¸€æ¬¡å¯†é’¥ï¼Œæˆ‘è¯¥å¦‚ä½•ä¿®æ”¹ä»£ç ï¼Œä½¿å¾—æ¯æ¬¡è°ƒç”¨apiçš„æ—¶å€™ï¼Œä¸éœ€è¦è¾“å…¥å¯†é’¥äº†\n",
      "\n",
      "åœ¨ä½¿ç”¨ OpenAI çš„ API æ—¶ï¼Œé€šå¸¸éœ€è¦æä¾› API å¯†é’¥æ¥è¿›è¡Œèº«ä»½éªŒè¯ã€‚å¦‚æœä½ å¸Œæœ›åœ¨æ¯æ¬¡è°ƒç”¨ API æ—¶ä¸éœ€è¦æ¯æ¬¡éƒ½è¾“å…¥å¯†é’¥ï¼Œå¯ä»¥è€ƒè™‘ä»¥ä¸‹å‡ ç§æ–¹æ³•ï¼š\n",
      "\n",
      "### 1. **ä½¿ç”¨ç¯å¢ƒå˜é‡å­˜å‚¨å¯†é’¥**\n",
      "   å°†ä½ çš„ API å¯†é’¥å­˜å‚¨åœ¨ç¯å¢ƒå˜é‡ä¸­ï¼Œè€Œä¸æ˜¯ç›´æ¥å†™åœ¨ä»£ç ä¸­ã€‚è¿™æ ·å¯ä»¥é¿å…å°†å¯†é’¥ç¡¬ç¼–ç åœ¨ä»£ç ä¸­ï¼Œæé«˜å®‰å…¨æ€§ã€‚\n",
      "\n",
      "   ```python\n",
      "   import os\n",
      "   from openai import OpenAI\n",
      "\n",
      "   # ä»ç¯å¢ƒå˜é‡ä¸­è·å– API å¯†é’¥\n",
      "   api_key = os.getenv(\"OPENAI_API_KEY\")\n",
      "\n",
      "   if not api_key:\n",
      "       raise ValueError(\"OPENAI_API_KEY environment variable is not set\")\n",
      "\n",
      "   client = OpenAI(api_key=api_key)\n",
      "   ```\n",
      "\n",
      "   ç„¶ååœ¨ä½ çš„ç¯å¢ƒä¸­è®¾ç½®ç¯å¢ƒå˜é‡ï¼š\n",
      "\n",
      "   ```bash\n",
      "   export OPENAI_API_KEY=\"your_api_key_here\"\n",
      "   ```\n",
      "\n",
      "   æˆ–è€…åœ¨ Windows ä¸Šï¼š\n",
      "\n",
      "   ```bash\n",
      "   set OPENAI_API_KEY=\"your_api_key_here\"\n",
      "   ```\n",
      "\n",
      "### 2. **ä½¿ç”¨é…ç½®æ–‡ä»¶**\n",
      "   å°† API å¯†é’¥å­˜å‚¨åœ¨é…ç½®æ–‡ä»¶ä¸­ï¼Œè€Œä¸æ˜¯ç›´æ¥å†™åœ¨ä»£ç ä¸­ã€‚\n",
      "\n",
      "   ```python\n",
      "   import configparser\n",
      "   from openai import OpenAI\n",
      "\n",
      "   config = configparser.ConfigParser()\n",
      "   config.read('config.ini')\n",
      "\n",
      "   api_key = config['OPENAI']['api_key']\n",
      "\n",
      "   if not api_key:\n",
      "       raise ValueError(\"API key not found in config file\")\n",
      "\n",
      "   client = OpenAI(api_key=api_key)\n",
      "   ```\n",
      "\n",
      "   åœ¨ `config.ini` æ–‡ä»¶ä¸­ï¼š\n",
      "\n",
      "   ```ini\n",
      "   [OPENAI]\n",
      "   api_key = your_api_key_here\n",
      "   ```\n",
      "\n",
      "### 3. **ä½¿ç”¨å¯†é’¥ç®¡ç†æœåŠ¡**\n",
      "   ä½¿ç”¨å¯†é’¥ç®¡ç†æœåŠ¡ï¼ˆå¦‚ AWS Secrets Managerã€Azure Key Vaultã€Google Cloud Secret Manager ç­‰ï¼‰æ¥å­˜å‚¨å’Œç®¡ç†ä½ çš„ API å¯†é’¥ã€‚è¿™æ ·å¯ä»¥é¿å…å°†å¯†é’¥ç¡¬ç¼–ç åœ¨ä»£ç ä¸­ã€‚\n",
      "\n",
      "   ```python\n",
      "   from google.oauth2 import service_account\n",
      "   from googleapiclient import discovery\n",
      "\n",
      "   # ä½¿ç”¨æœåŠ¡è´¦æˆ·å¯†é’¥\n",
      "   credentials = service_account.Credentials.from_service_account_file(\n",
      "       'path/to/service-account-file.json',\n",
      "       scopes=['https://www.googleapis.com/auth/cloud-platform']\n",
      "   )\n",
      "\n",
      "   # åˆ›å»ºæœåŠ¡å¯¹è±¡\n",
      "   client = discovery.build('language', 'v1beta1', credentials=credentials)\n",
      "\n",
      "   # ä½¿ç”¨æœåŠ¡å¯¹è±¡è¿›è¡Œè°ƒç”¨\n",
      "   response = client.language().classify(body={'document': 'your document here'}).execute()\n",
      "   ```\n",
      "\n",
      "### 4. **ä½¿ç”¨ç¯å¢ƒå˜é‡å’Œé…ç½®æ–‡ä»¶ç»“åˆ**\n",
      "   ä½ å¯ä»¥ç»“åˆä½¿ç”¨ç¯å¢ƒå˜é‡å’Œé…ç½®æ–‡ä»¶ï¼Œå°†ç¯å¢ƒå˜é‡ä½œä¸ºä¼˜å…ˆçº§è¾ƒé«˜çš„é…ç½®ã€‚\n",
      "\n",
      "   ```python\n",
      "   import os\n",
      "   from configparser import ConfigParser\n",
      "\n",
      "   config = ConfigParser()\n",
      "   config.read('config.ini')\n",
      "\n",
      "   api_key = config['OPENAI']['api_key']\n",
      "\n",
      "   if not api_key:\n",
      "       api_key = os.getenv(\"OPENAI_API_KEY\")\n",
      "\n",
      "   if not api_key:\n",
      "       raise ValueError(\"API key not found in config file or environment variable\")\n",
      "\n",
      "   client = OpenAI(api_key=api_key)\n",
      "   ```\n",
      "\n",
      "### 5. **ä½¿ç”¨å¯†é’¥ç®¡ç†æœåŠ¡ï¼ˆå¦‚ AWS Secrets Managerï¼‰**\n",
      "   å¦‚æœä½ ä½¿ç”¨çš„æ˜¯ AWSï¼Œå¯ä»¥ä½¿ç”¨ AWS Secrets Manager æ¥å­˜å‚¨å’Œç®¡ç†ä½ çš„ API å¯†é’¥ã€‚\n",
      "\n",
      "   ```python\n",
      "   import boto3\n",
      "\n",
      "   client = boto3.client('secretsmanager')\n",
      "\n",
      "   response = client.get_secret_value(SecretId='your_secret_id')\n",
      "\n",
      "   secret = response['SecretString']\n",
      "   secret_dict = secret.json()\n",
      "\n",
      "   api_key = secret_dict['api_key']\n",
      "\n",
      "   client = OpenAI(api_key=api_key)\n",
      "   ```\n",
      "\n",
      "### 6. **ä½¿ç”¨å¯†é’¥ç®¡ç†æœåŠ¡ï¼ˆå¦‚ Azure Key Vaultï¼‰**\n",
      "   å¦‚æœä½ ä½¿ç”¨çš„æ˜¯ Azureï¼Œå¯ä»¥ä½¿ç”¨ Azure Key Vault æ¥å­˜å‚¨å’Œç®¡ç†ä½ çš„ API å¯†é’¥ã€‚\n",
      "\n",
      "   ```python\n",
      "   from azure.identity import DefaultAzureCredential\n",
      "   from azure.keyvault.secrets import SecretClient\n",
      "\n",
      "   credential = DefaultAzureCredential()\n",
      "   client = SecretClient(vault_url=\"https://your-key-vault-url.vault.azure.net/\", credential=credential)\n",
      "\n",
      "   secret = client.get_secret(\"your_secret_name\")\n",
      "   api_key = secret.value\n",
      "\n",
      "   client = OpenAI(api_key=api_key)\n",
      "   ```\n",
      "\n",
      "### 7. **ä½¿ç”¨å¯†é’¥ç®¡ç†æœåŠ¡ï¼ˆå¦‚ Google Cloud Secret Managerï¼‰**\n",
      "   å¦‚æœä½ ä½¿ç”¨çš„æ˜¯ Google Cloudï¼Œå¯ä»¥ä½¿ç”¨ Google Cloud Secret Manager æ¥å­˜å‚¨å’Œç®¡ç†ä½ çš„ API å¯†é’¥ã€‚\n",
      "\n",
      "   ```python\n",
      "   from google.oauth2 import service_account\n",
      "   from googleapiclient import discovery\n",
      "\n",
      "   credentials = service_account.Credentials.from_service_account_file(\n",
      "       'path/to/service\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": \"\"\"\n",
    "* * * * * * 1 0\n",
    "* * * 3 2 1 1 0\n",
    "* * * 2 0 0 0 0\n",
    "* * * 2 0 0 0 0\n",
    "1 2 1 1 1 1 1 0\n",
    "F 1 0 0 1 F 2 1\n",
    "1 1 0 0 1 3 F 2\n",
    "0 0 0 0 0 2 F *\n",
    "\"\"\"}\n",
    "]\n",
    "\n",
    "# Apply chat template, enabling thinking mode\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False,\n",
    ")\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate\n",
    "generated_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=1024,\n",
    ")\n",
    "\n",
    "# Extract only the new tokens\n",
    "output_ids = generated_ids[0][len(inputs.input_ids[0]):].tolist()\n",
    "decoded_output = tokenizer.decode(output_ids, skip_special_tokens=True).strip(\"\\n\")\n",
    "print(\"output:\", decoded_output)\n",
    "\n",
    "# # Extract solution block\n",
    "# import re\n",
    "# match = re.search(r\"<SOLUTION>(.*?)</SOLUTION>\", decoded_output, re.DOTALL)\n",
    "# if match:\n",
    "#     move = match.group(1).strip()\n",
    "# else:\n",
    "#     move = decoded_output  # fallback if tags missing\n",
    "\n",
    "# print(\"final move:\", move)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MinesweeperGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
